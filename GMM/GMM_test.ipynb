{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM算法测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 来自于mujoco150在win+py3.9下的矫情的要求 --------\n",
    "# 手动添加mujoco路径\n",
    "import os\n",
    "from getpass import getuser\n",
    "user_id = getuser()\n",
    "os.add_dll_directory(f\"C://Users//{user_id}//.mujoco//mujoco200//bin\")\n",
    "os.add_dll_directory(f\"C://Users//{user_id}//.mujoco//mujoco-py-2.0.2.0//mujoco_py\")\n",
    "# -------------------------------------------------------\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import gym_custom_env       # 注册自定义环境\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "\n",
    "from stable_baselines3 import SAC, TD3, PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from:\n",
      "best_model/5e6_steps_t5_cpu8_sac_HumanoidCustomEnv-v0.zip\n",
      "Compiling d:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\mujoco_py\\cymj.pyx because it changed.\n",
      "[1/1] Cythonizing d:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\mujoco_py\\cymj.pyx\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def update_xml_model(self,params):\n",
    "    # VecEnv更新XML模型\n",
    "    for env_idx in range(self.num_envs):\n",
    "        self.envs[env_idx].update_xml_model(params)\n",
    "    \n",
    "\n",
    "DummyVecEnv.update_xml_model = update_xml_model\n",
    "print('load from:')\n",
    "save_path = 'best_model/5e6_steps_t5_cpu8_sac_HumanoidCustomEnv-v0.zip'\n",
    "print(save_path)\n",
    "env_kwargs = {'terrain_type':'steps'}\n",
    "env_model = make_vec_env(env_id = 'HumanoidCustomEnv-v0', n_envs = 5, env_kwargs = env_kwargs)\n",
    "algo = 'sac'\n",
    "hyperparams = {\n",
    "    \"sac\": dict(\n",
    "        batch_size=256,\n",
    "        gamma=0.98,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        learning_starts=10000,\n",
    "        buffer_size=int(5e2),\n",
    "        tau=0.01,\n",
    "        gradient_steps=4,\n",
    "    ),\n",
    "    \"ppo\": dict(\n",
    "        batch_size=512,\n",
    "        learning_rate=2.5e-4,\n",
    "        policy_kwargs=dict(net_arch=({'pi':[128,128]},{'vf':[128,128]})),\n",
    "        gamma=0.99\n",
    "    )\n",
    "}[algo]\n",
    "model = SAC(\"MlpPolicy\", env_model, verbose=1,  **hyperparams)\n",
    "model.set_parameters(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n",
      "============ HUMANOID CUSTOM ENV ============\n",
      "=====terrain type:steps=====\n"
     ]
    }
   ],
   "source": [
    "from GMM_try import GMM_Design_Optim\n",
    "\n",
    "GMM_optimizer = GMM_Design_Optim(model,n_generations=10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thigh_lenth': 0.34081252582239346,\n",
       " 'shin_lenth': 0.2991977227307777,\n",
       " 'upper_arm_lenth': 0.26948172978172574,\n",
       " 'lower_arm_lenth': 0.2890919759244781,\n",
       " 'foot_lenth': 0.18037569801834527}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GMM_optimizer.evolve()\n",
    "GMM_optimizer.get_gmm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = [ .7, .7 ,.7, .7, 0.7]\n",
    "# GMM_optimizer.update_gmm(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM_optimizer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "2\n",
      "7\n",
      "0\n",
      "1\n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n",
      "----- Update XML Model ----- \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m GMM_optimizer\u001b[39m.\u001b[39;49mevolve()\n",
      "File \u001b[1;32md:\\0PersonalFiles\\gym_RE-master\\GMM_try.py:227\u001b[0m, in \u001b[0;36mGMM_Design_Optim.evolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    225\u001b[0m generated_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_gmm(optimized_params_coef)\n\u001b[0;32m    226\u001b[0m x_train \u001b[39m=\u001b[39m generated_samples\n\u001b[1;32m--> 227\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_fitness(generated_samples)])  \u001b[39m#负数越小越好\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39mprint\u001b[39m(y_train[\u001b[39m0\u001b[39m])\n\u001b[0;32m    229\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_y))\n",
      "File \u001b[1;32md:\\0PersonalFiles\\gym_RE-master\\GMM_try.py:193\u001b[0m, in \u001b[0;36mGMM_Design_Optim.get_fitness\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_fitness\u001b[39m(\u001b[39mself\u001b[39m, params):\n\u001b[1;32m--> 193\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrobot_loss(params)\n\u001b[0;32m    194\u001b[0m     \u001b[39m# return pred - np.min(pred)+1e-3\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m pred\n",
      "File \u001b[1;32md:\\0PersonalFiles\\gym_RE-master\\GMM_try.py:161\u001b[0m, in \u001b[0;36mGMM_Design_Optim.robot_loss\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mwhile\u001b[39;00m episode \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs:\n\u001b[0;32m    160\u001b[0m     action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(obs, )\n\u001b[1;32m--> 161\u001b[0m     obs, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    162\u001b[0m     episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m    163\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\gym_RE\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32md:\\0PersonalFiles\\gym_RE-master\\gym_custom_env\\humanoidCustom.py:643\u001b[0m, in \u001b[0;36mHumanoidCustomEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    641\u001b[0m forward_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_reward\n\u001b[0;32m    642\u001b[0m healthy_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhealthy_reward\n\u001b[1;32m--> 643\u001b[0m posture_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposture_reward\n\u001b[0;32m    644\u001b[0m contact_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontact_reward\n\u001b[0;32m    646\u001b[0m rewards \u001b[39m=\u001b[39m forward_reward \u001b[39m+\u001b[39m healthy_reward \u001b[39m+\u001b[39m posture_reward \u001b[39m+\u001b[39m contact_reward\n",
      "File \u001b[1;32md:\\0PersonalFiles\\gym_RE-master\\gym_custom_env\\humanoidCustom.py:200\u001b[0m, in \u001b[0;36mHumanoidCustomEnv.posture_reward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m# 提取躯干的z轴方向向量\u001b[39;00m\n\u001b[0;32m    199\u001b[0m vertical_direction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m--> 200\u001b[0m body_z_axis \u001b[39m=\u001b[39m rotation_matrix\u001b[39m.\u001b[39;49mdot(vertical_direction)\n\u001b[0;32m    201\u001b[0m \u001b[39m# 计算z轴方向向量与竖直向上方向向量的点积\u001b[39;00m\n\u001b[0;32m    202\u001b[0m z_dot_product \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(body_z_axis, vertical_direction)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GMM_optimizer.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_optimizer.save_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA\\envs\\GYM\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGMM_try\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GMM_Design_Optim\n\u001b[0;32m      3\u001b[0m GMM_optimizer \u001b[38;5;241m=\u001b[39m GMM_Design_Optim(model,n_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32me:\\CASIA\\gym_RobotEvolution\\GMM\\GMM_try.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvec_env\u001b[39;00m \u001b[39mimport\u001b[39;00m DummyVecEnv\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdoctest\u001b[39;00m \u001b[39mimport\u001b[39;00m master\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SGDRegressor\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixture\u001b[39;00m \u001b[39mimport\u001b[39;00m GaussianMixture\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "GMM_optimizer.get_gmm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('GYM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "885ba80e3279e9342e7dd7c1f03a1cf54dcbcf219785bf65aec8a278e13e1341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
